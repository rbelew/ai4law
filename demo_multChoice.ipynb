{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c0dff0-63fe-4dfd-940c-785abbb8b704",
   "metadata": {},
   "source": [
    "# demo_multChoice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a60cb-f4ed-44ea-99c1-c8506acaae0b",
   "metadata": {},
   "source": [
    "This demo code owes much to that provided by [Lucia Zheng here](https://github.com/reglab/casehold/tree/main/multiple_choice), and also to hints from this [Captum tutorial](https://captum.ai/tutorials/Bert_SQUAD_Interpret).  \n",
    "\n",
    "It has been stream-lined somewhat, eg, by dropping some options (eg, Tensorflow, computing pre-training loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5e062-3f89-40d0-a658-cf1c808141cb",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Both data and models are [made available via the casehold account at Hugging Face](https://huggingface.co/casehold).\n",
    "\n",
    "In general, you'll probably want to cache these files on some local machine.\n",
    "Two parameters connect this code to your local environment:\n",
    "\n",
    "- `DataDir`: containing training and testing examples\n",
    "- `ModelPath`: points to the cached source for the models\n",
    "\n",
    "These parameters are set below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ceae1-fc73-4986-b18f-c4b381fb982c",
   "metadata": {},
   "source": [
    "## package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d229c6-e1c6-4ff2-a91f-a45710a5e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import evaluate\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import datasets.utils\n",
    "# Support for load_metric has been removed in datasets@3.0.0, see Release 3.0.0 Â· huggingface/datasets\n",
    "# from datasets import load_metric\n",
    "\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import  check_min_version\n",
    "import transformers.utils\n",
    "\n",
    "import captum\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    ShapleyValues,\n",
    "    LayerIntegratedGradients, \n",
    "    LLMAttribution, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    "    TextTemplateInput,\n",
    "    ProductBaselines,\n",
    "    LayerConductance,\n",
    "    TokenReferenceBase\n",
    ")\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb525f9b-5608-48ee-92cd-704c9f7825fb",
   "metadata": {},
   "source": [
    "## Multiple choice utilities from https://github.com/reglab/casehold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42b130-41c1-4ee3-8620-8dc7a755cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_casehold import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2eac59-d81d-4394-9c66-454aee07f40a",
   "metadata": {},
   "source": [
    "## important constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb81a8-cc58-474c-aa71-12e0d7e6593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "HOST = socket.gethostname()\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.40.0.dev0\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "processors = {\"casehold\": CaseHOLDProcessor}\n",
    "\n",
    "NUM_MULTIPLE_CHOICE_LABELS = 5\n",
    "\n",
    "Verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc9f96-f993-4861-8b73-619ff08897a7",
   "metadata": {},
   "source": [
    "## utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f9ce3-9179-4fd4-9554-9ab6f9a2ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9\n",
    "def count_TrainParam(model: torch.nn.Module) -> int:\n",
    "    \"\"\" Returns the number of learnable parameters for a PyTorch model \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_AllParam(model: torch.nn.Module) -> int:\n",
    "    \"\"\" Returns the number of learnable parameters for a PyTorch model \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def tensor2scalar(t):\n",
    "    return t.cpu().detach().numpy().tolist()\n",
    "\n",
    "def tensor2list(t):\n",
    "    return t.cpu().detach().numpy().tolist()[0]\n",
    "\n",
    "def roundAttrib(atensor):\n",
    "    '''convert float to restricted range of strings\n",
    "    '''\n",
    "    a = tensor2scalar(atensor)\n",
    "    return f'{a:.2f}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07458651-d5fe-4410-9ea1-c3026b3714e4",
   "metadata": {},
   "source": [
    "## Transformer classes/functions\n",
    "\n",
    "These patterns seem common across transformer demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5026a9-8940-434f-b86f-0a358fd8a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str = field(metadata={\"help\": \"The name of the task to train on: \" + \", \".join(processors.keys())})\n",
    "    data_dir: str = field(metadata={\"help\": \"Should contain the data files for the task.\"})\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    token: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n",
    "                \"generated when running `huggingface-cli login` (stored in `~/.huggingface`).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\"\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n",
    "                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n",
    "                \"execute code present on the Hub on your local machine.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    \n",
    "# from https://github.com/reglab/casehold/blob/main/multiple_choice/run_multiple_choice.py\n",
    "# Define custom compute_metrics function, returns macro F1 metric for CaseHOLD task\n",
    "EvalF1Metric = evaluate.load('f1')\n",
    "def compute_metrics_f1(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    # metric = load_metric(\"f1\")\n",
    "    metric = EvalF1Metric\n",
    "    # Compute macro F1 for 5-class CaseHOLD task\n",
    "    f1 = metric.compute(predictions=preds, references=p.label_ids, average='macro')\n",
    "    return f1\n",
    "\n",
    "# from https://captum.ai/tutorials/Bert_SQUAD_Interpret\n",
    "def multChoice_forward(model,inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    prob = torch.softmax(output.logits,1)\n",
    "    # targetIdx = tensor2list(prob.argmax(1))\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce3b76-e9c7-4d05-b03e-1d668cd24150",
   "metadata": {},
   "source": [
    "## Captum hooks\n",
    "\n",
    "Most of the work is done in the `captumIG` function. It has been written assuming it is given a model and tokenizer, a list of examples and tests.  It produces several results, written to a directory.\n",
    "\n",
    "It makes use of a function `vis_text2` which is a modified version of captum's (`visualization.vis_text`)[https://github.com/pytorch/captum/blob/master/captum/attr/_utils/visualization.py] function, \"...partially copied from experiments conducted by Davide Testuggine at Facebook.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d799aea-63ae-4fce-9ae9-08903194d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_text2(\n",
    "    datarecords: Iterable[viz.VisualizationDataRecord], idx, legend: bool = False\n",
    ") -> \"HTML\":  # In quotes because this type doesn't exist in standalone mode\n",
    "    assert HAS_IPYTHON, (\n",
    "        \"IPython must be available to visualize text. \"\n",
    "        \"Please run 'pip install ipython'.\"\n",
    "    )\n",
    "    dom = [] # [\"<table width: 100%>\"]\n",
    "    \n",
    "    rows = [\n",
    "        \"<tr>\"\n",
    "        \"<th>Idx</th>\"\n",
    "        \"<th>True Label</th>\"\n",
    "        \"<th>Predicted Label</th>\"\n",
    "        \"<th>Attribution Label</th>\"\n",
    "        \"<th>Attribution Score</th>\"\n",
    "        \"<th>Word Importance</th>\"\n",
    "        \"</tr>\\n\"\n",
    "    ]\n",
    "    rows = []\n",
    "    for datarecord in datarecords:\n",
    "        rows.append(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    \"<tr>\",\n",
    "                    f'<td>{idx}</td>',\n",
    "                    viz.format_classname(datarecord.true_class),\n",
    "                    viz.format_classname(\n",
    "                        \"{0} ({1:.2f})\".format(\n",
    "                            datarecord.pred_class, datarecord.pred_prob\n",
    "                        )\n",
    "                    ),\n",
    "                    viz.format_classname(datarecord.attr_class),\n",
    "                    viz.format_classname(\"{0:.2f}\".format(datarecord.attr_score)),\n",
    "                    viz.format_word_importances(\n",
    "                        datarecord.raw_input_ids, datarecord.word_attributions\n",
    "                    ),\n",
    "                    \"</tr>/n\",\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if legend:\n",
    "        dom.append(\n",
    "            '<div style=\"border-top: 1px solid; margin-top: 5px; \\\n",
    "            padding-top: 5px; display: inline-block\">'\n",
    "        )\n",
    "        dom.append(\"<b>Legend: </b>\")\n",
    "\n",
    "        for value, label in zip([-1, 0, 1], [\"Negative\", \"Neutral\", \"Positive\"]):\n",
    "            dom.append(\n",
    "                '<span style=\"display: inline-block; width: 10px; height: 10px; \\\n",
    "                border: 1px solid; background-color: \\\n",
    "                {value}\"></span> {label}  '.format(\n",
    "                    value=viz._get_color(value), label=label\n",
    "                )\n",
    "            )\n",
    "        dom.append(\"</div>\")\n",
    "\n",
    "    dom.append(\"\".join(rows))\n",
    "    # dom.append(\"</table>\")\n",
    "    html = HTML(\"\".join(dom))\n",
    "    display(html)\n",
    "\n",
    "    return html\n",
    "\n",
    "# https://github.com/pytorch/captum/blob/master/tutorials/Bert_SQUAD_Interpret.ipynb\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "def captumIG(model,tokenizer,testList,targetList,outdir):\n",
    "    '''captum integrated gradiant attribution\n",
    "        for multiple choice\n",
    "    '''\n",
    "    \n",
    "    # from https://github.com/pytorch/captum/issues/303\n",
    "    # https://github.com/pytorch/captum/blob/master/tutorials/Bert_SQUAD_Interpret.ipynb\n",
    "    MCfwd = partial(multChoice_forward,model)\n",
    "\n",
    "    lig = LayerIntegratedGradients(MCfwd, model.bert.embeddings)\n",
    "\n",
    "    rptFile = outdir + 'stats.csv'\n",
    "    outs = open(rptFile,'w')\n",
    "    outs.write('Idx,target,predLbl,correct,predProb,attrSum\\n')\n",
    "    allHTML = ''\n",
    "    \n",
    "    CaptumTblHdr = '''<table width: 100%>\n",
    "      <tr><th>Idx</th><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th>'''\n",
    "\n",
    "    CaptumTblTrlr = '</table>\\n'\n",
    "    \n",
    "    allHTML += CaptumTblHdr\n",
    "    \n",
    "    tokFreq = defaultdict(lambda: defaultdict(int)) # inputIdx -> roundAttrib -> freq\n",
    "    \n",
    "    for egIdx, tst in enumerate(testList):\n",
    "        # NB: use parallel structure of testList, targetList\n",
    "        \n",
    "        # 241207: vector of prob\n",
    "        # targetVec = targetList[egIdx][0]\n",
    "        # targetIdx = tensor2scalar(targetVec.argmax())\n",
    "        \n",
    "        targetIdx = tensor2scalar(targetList[egIdx])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        out = model(**tst)\n",
    "        \n",
    "        # interpret logits\n",
    "        logits = out.logits    \n",
    "        prob = torch.softmax(logits,1)\n",
    "        predIdx = tensor2list(prob.argmax(1))\n",
    "        predProb = tensor2list(prob.max(1)[0])\n",
    "        correct = 1 if predIdx == targetIdx else 0\n",
    "        print(f'captumIG: {egIdx=} {targetIdx=} {predIdx=} {correct} {predProb=}')    \n",
    "        \n",
    "        tstEGTuple = (tst['input_ids'], \n",
    "                      tst['attention_mask'], \n",
    "                      tst['token_type_ids'])\n",
    "            \n",
    "        attributions_ig = lig.attribute(tstEGTuple, n_steps=5,target=targetIdx) \n",
    "        \n",
    "        # attributions_sum has shape [5,128]\n",
    "        attributions_sum = summarize_attributions(attributions_ig)\n",
    "    \n",
    "        # 241208: focus on predicted; also target if different?\n",
    "        predAttrib = attributions_sum[predIdx]\n",
    "        \n",
    "        # NB: need tyo get FIRST element of tst['input_ids']?\n",
    "        indices = tst['input_ids'][0][predIdx].detach().tolist()\n",
    "        # indices has all NUM_MULTIPLE_CHOICE_LABELS choices\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "        \n",
    "        # collect token attributions across all examples\n",
    "        for i,tokIdx in enumerate(indices):\n",
    "            if tokIdx == PAD_IDX:\n",
    "                continue\n",
    "            tokAttrib = predAttrib[i]\n",
    "            # NB: round floats to create small set of keys\n",
    "            tokarnd = roundAttrib(tokAttrib)\n",
    "            tokFreq[tokIdx][tokarnd] += 1\n",
    "                \n",
    "        delta_start = 0.\n",
    "    \n",
    "        # cf. viz.VisualizationDataRecord.__init__()\n",
    "        vis = viz.VisualizationDataRecord(\n",
    "                            predAttrib,        #    word_attributions: Tensor,\n",
    "                            predProb,                #     pred_prob: float,\n",
    "                            predIdx,                #     pred_class: int,\n",
    "                            targetIdx,                    #     true_class: int,\n",
    "                            str(targetIdx),            #     attr_class: int,\n",
    "                            predAttrib.sum(), #     attr_score: float,\n",
    "                            all_tokens,                #     raw_input_ids: List[str],\n",
    "                            delta_start)            #     convergence_score: float,\n",
    "    \n",
    "        visHTML = vis_text2([vis],egIdx)\n",
    "        visTxt = visHTML.data\n",
    "        allHTML += visTxt\n",
    "        \n",
    "        outs.write(f'{egIdx},{targetIdx},{predIdx},{correct},\"{predProb}\",{predAttrib.sum()}\\n')\n",
    "    \n",
    "    outs.close()\n",
    "    \n",
    "    vizPath = outdir+'viz.html'\n",
    "    allHTML += CaptumTblTrlr\n",
    "    outs = open(vizPath,'w')\n",
    "    outs.write(allHTML)\n",
    "    print(f'# viz written to {vizPath}')\n",
    "    outs.close()\n",
    "\n",
    "    allAttribSet = set()\n",
    "    for tokIdx in tokFreq.keys():\n",
    "        for attrib in tokFreq[tokIdx].keys():\n",
    "            allAttribSet.add(attrib)\n",
    "    print(f'captumEG: Total attributes={len(allAttribSet)}')\n",
    "    allAttrib = sorted(list(allAttribSet))\n",
    "    \n",
    "    # NB: NEGATIVE attrib score STRINGS in REVERSED order\n",
    "    firstPos = None\n",
    "    for i,a in enumerate(allAttrib):\n",
    "        if not a.startswith('-'):\n",
    "            firstPos = i\n",
    "            break\n",
    "    negAttrib = list(reversed(allAttrib[:firstPos]))\n",
    "    allAttrib = negAttrib + allAttrib[firstPos:]\n",
    "    \n",
    "    tokFreqFile = outdir + 'tokfreq.csv'\n",
    "    outs = open(tokFreqFile,'w')\n",
    "    hdr = 'Idx,Token,SigNeg,SigPos,NUAttrib'\n",
    "    for a in allAttrib:\n",
    "        hdr += f',{a}'\n",
    "    outs.write(hdr+',TotFreq\\n')\n",
    "    \n",
    "    signifThresh = 0.05\n",
    "    for tokIdx in sorted(list(tokFreq.keys())):\n",
    "        tok = tokenizer.convert_ids_to_tokens([tokIdx])[0]\n",
    "        if tok=='\"':\n",
    "            tok = '\"\"'\n",
    "        nsignifPos = 0\n",
    "        nsignifNeg = 0\n",
    "        \n",
    "        for attrib in tokFreq[tokIdx]:\n",
    "            fa = float(attrib)\n",
    "            if fa < -signifThresh:\n",
    "                nsignifNeg += 1\n",
    "            elif fa > signifThresh:\n",
    "                nsignifPos += 1\n",
    "                \n",
    "        line = f'{tokIdx},\"{tok}\",{nsignifNeg},{nsignifPos},{len(tokFreq[tokIdx])}'\n",
    "        tot = 0 \n",
    "        for attrib in allAttrib:\n",
    "            # total frequency weighted ABSOLUTE attribution\n",
    "            if attrib in tokFreq[tokIdx]:\n",
    "                line += f',{tokFreq[tokIdx][attrib]}'\n",
    "                # \n",
    "                tot += tokFreq[tokIdx][attrib] * abs(float(attrib))\n",
    "            else:\n",
    "                line += ', '\n",
    "        line += f',{tot}'\n",
    "        outs.write(line+'\\n')\n",
    "    outs.close()        \n",
    "    print('captumEG: done')\n",
    "    \n",
    "    return allHTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd5208-8342-4a71-9205-22036035bf9f",
   "metadata": {},
   "source": [
    "## Ready to get some work done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47517203-835d-468b-8301-2b511029f543",
   "metadata": {},
   "source": [
    "### Different machines\n",
    "\n",
    "I use a couple of different machines. The default for this demo is to assume the MPS library `(platform.platform()=\"macOS-14.7-arm64-arm-64bit\")` and using `DEVICE='mps'`. It has also been run on a Linux machine with a NVidia GV??? \n",
    "\n",
    "Modify `BatchSize` to fit your GPU capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6bd97-16b2-4228-8ac2-6e32a1f0423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOST.startswith('wayne'):\n",
    "    # DEVICE='cpu'\n",
    "    DEVICE='mps'\n",
    "    DataDir = '/Users/rik/.cache/'        \n",
    "    OutDir = '/Users/rik/data/ai4law/'\n",
    "    ModelPath = '/Users/rik/.cache/casehold-models/'\n",
    "    BatchSize = 16\n",
    "elif HOST=='mjq':\n",
    "    DEVICE='cuda'\n",
    "    DataDir =  '/home/rik/.cache/'\n",
    "    OutDir = '/home/Data/ai4law/'\n",
    "    ModelPath = '/home/Data/ai4lawData/casehold-models/'\n",
    "    BatchSize = 12\n",
    "else:\n",
    "    assert False, f'Unknown host?! {HOST}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91aee8f-5df0-47e4-8a6d-da0688a0a7e8",
   "metadata": {},
   "source": [
    "### Echo some platform specific details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50730847-f53b-4fe9-b572-7c8aec911d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# {HOST=} {DEVICE=}')\n",
    "print(f'# platform={platform.platform()}') # macOS-14.7-arm64-arm-64bit\n",
    "print(f'# MPS backend: {torch.backends.mps.is_available()}')\n",
    "for package in (torch, transformers,captum):\n",
    "    print('#\\t',package.__name__, package.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970750aa-c388-4047-8bec-b3e1123cce2a",
   "metadata": {},
   "source": [
    "### Identify model and other parameters for transformer\n",
    "\n",
    "To make this python module self-contained, arguments that the casehold demo made shell argument are explicitly contructed as a list.\n",
    "\n",
    "This demo assumes locally cached models.\n",
    "\n",
    "Note that the data cache is being over-written, forcing the SWAG-style `MultipleChoiceDataset.convert_examples_to_features()` to be rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae2096-e682-4909-b3c6-a3434d4cb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelName = 'legalbert' # 'legalbert' 'custom-legalbert' 'bert-double'\n",
    "ModelPath += f'{ModelName}/'\n",
    "\n",
    "DataSet = 'casehold'\n",
    "DataSetTags = 'casehold'\n",
    "DataDir += 'datasets/casehold___casehold/all/direct/'\n",
    "\n",
    "Trained = 'train' # 'train' 'useTrained'\n",
    "\n",
    "RunName = f'{ModelName}_{DataSet}_{Trained}'\n",
    "OutDir += f'{RunName}/'\n",
    "\n",
    "print(f'# {ModelPath=}\\n# {RunName=}\\n# {DataDir=}\\n# {OutDir=} {Trained=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f345a-48bc-4447-bc27-ad75e998017a",
   "metadata": {},
   "source": [
    "args is a used as a module parameter, rather than via a call to python as implemented by the [casehold demo](https://github.com/reglab/casehold/blob/main/demo.ipynb). `HfArgumentParser` is used to break these into sets for model, data and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdf09c-39cf-4813-914c-7c7924440496",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = []\n",
    "\n",
    "if Trained=='train':\n",
    "    args.append('--do_train')\n",
    "    \n",
    "args.append('--model_name_or_path');  args.append(ModelPath)\n",
    "\n",
    "args.append('--data_dir');            args.append(DataDir)\n",
    "args.append('--task_name');           args.append('casehold')\n",
    "\n",
    "args.append('--output_dir');          args.append(OutDir)\n",
    "\n",
    "args.append('--overwrite_cache=True')\n",
    "\n",
    "args.append('--max_seq_length');      args.append('128')\n",
    "args.append('--do_eval')\n",
    "args.append('--eval_strategy'); args.append('steps')\n",
    "\n",
    "args.append(f'--per_device_train_batch_size={BatchSize}')\n",
    "args.append(f'--per_device_eval_batch_size={BatchSize}')\n",
    "    \n",
    "args.append('--learning_rate=1e-5')\n",
    "args.append('--num_train_epochs=2.0')\n",
    "args.append('--overwrite_output_dir=True')\n",
    "args.append('--logging_steps');       args.append('50')\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "       \n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b3789-7707-4d21-8b6c-fc912b68688a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Set up logging, checkpoints, config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1df0f-4264-4e39-9adb-10f95a8a9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OutDir):\n",
    "    os.makedirs(OutDir)\n",
    "    \n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=OutDir+f'{RunName}.log',\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
    "    + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a8dc8-e85c-4eb3-a46b-0f8bc9817a52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get the model and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d7c43-9d9a-4096-94db-5e8b4a7838c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    ")\n",
    "        \n",
    "model = AutoModelForMultipleChoice.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    token=model_args.token,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "global PAD_IDX\n",
    "global CLS_IDX\n",
    "global SEP_IDX\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "CLS_IDX = tokenizer.cls_token_id\n",
    "SEP_IDX = tokenizer.sep_token_id\n",
    "\n",
    "# ValueError: You are trying to save a non contiguous tensor:      \n",
    "# https://github.com/huggingface/transformers/issues/28293#issuecomment-2284567863\n",
    "for param in model.parameters(): param.data = param.data.contiguous()\n",
    "\n",
    "if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the \"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "    )\n",
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e585db-d775-48e3-b658-e298319e514f",
   "metadata": {},
   "source": [
    "### Establish training and evaluation datasets\n",
    "\n",
    "Note that the cache is being over-written, forcing the SWAG-style `MultipleChoiceDataset.convert_examples_to_features()` to be rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc19507-0db5-4ee5-be97-c5b8bd129686",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \\\n",
    "    MultipleChoiceDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        task=data_args.task_name,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.train,\n",
    "    )\n",
    "\n",
    "eval_dataset = \\\n",
    "    MultipleChoiceDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        task=data_args.task_name,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e63b48-55be-442b-848d-b1dcc9fa7b9b",
   "metadata": {},
   "source": [
    "For TESTING, you can identify subset of eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d516e-7406-4441-9d4c-7ca703d19c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caseholdSamples = [0,1,2,3,4]\n",
    "# tstEG = [eval_dataset[i] for i in caseholdSamples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7eddd7-88cc-4954-ad28-5a41ea56b16d",
   "metadata": {},
   "source": [
    "### Create modified versions of examples for various purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f5c30-eb27-460b-a73d-3ace70095252",
   "metadata": {},
   "outputs": [],
   "source": [
    "inBits = ['input_ids','attention_mask','token_type_ids']\n",
    "\n",
    "# dict of tensors works with model\n",
    "eval_TensorDict = []\n",
    "for eg in eval_dataset:\n",
    "    d = {}\n",
    "    for k in inBits:\n",
    "        d[k] =  torch.tensor( eg.__getattribute__(k),device=DEVICE).unsqueeze(0)\n",
    "    eval_TensorDict.append(d)\n",
    "                \n",
    "# tuple of dicts works for SummaryWriter.add_graph()\n",
    "eval_TensorTuple = [ (d['input_ids'], \n",
    "                      d['attention_mask'], \n",
    "                      d['token_type_ids']) for d in eval_TensorDict]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd037744-fde5-472b-acf3-82fdc6fd5e62",
   "metadata": {},
   "source": [
    "### Create vector of examples' target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c03514-c699-4771-b1d3-43124197cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxTbl = {}\n",
    "egTarget = []\n",
    "for i,tst in enumerate(eval_dataset):\n",
    "    idxTbl[i] = tst.example_id\n",
    "    targetIdx = tst.label\n",
    "   \n",
    "    # 241207: NO: target should NOT be vector of probabilities?\n",
    "    # target4captum = [ torch.tensor([1. if i == targetIdx else 0. for i in range(NUM_MULTIPLE_CHOICE_LABELS)]) ]\n",
    "\n",
    "    # captum/attri/_core/integrated_gradient.py L#168\n",
    "    #\n",
    "    # - target (int, tuple, Tensor, or list, optional): Output indices\n",
    "    # for which gradients are computed (for classification cases, this\n",
    "    # is usually the target class).\n",
    "    target4captum = torch.tensor(targetIdx,device=DEVICE)\n",
    "    \n",
    "    egTarget.append( target4captum )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04d515-6886-4635-bc85-daa51515b3db",
   "metadata": {},
   "source": [
    "### Produce details of network for use by tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576a94a-2c40-4fc4-a074-82c04120c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(OutDir + f'runs/summary/')\n",
    "# NB: assumes args (tuple): input tensor[s] for the model\n",
    "    \n",
    "writer.add_graph(model, eval_TensorTuple[0], use_strict_trace=False)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb4a95-8b11-4c22-8ce1-b1275b455b29",
   "metadata": {},
   "source": [
    "### Train the model!\n",
    "\n",
    "This takes several hours on my machines.  The trainer also produces periodic evaluations of the model every 50 steps (batches of training examples) during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9683514d-b293-4c65-93be-affeff855b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_f1,\n",
    ")\n",
    "\n",
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "metrics = train_result.metrics\n",
    "\n",
    "metrics[\"train_samples\"] = len(train_dataset)\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b31e4-f5b4-43a8-bfc9-e1a2a178f052",
   "metadata": {},
   "source": [
    "### Document our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce2e80-9706-4718-83ca-a28ab4f7ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"finetuned_from\": model_args.model_name_or_path,\n",
    "    \"tasks\": \"multiple-choice\",\n",
    "    \"dataset_tags\": DataSetTags,\n",
    "    \"dataset_args\": \"regular\",\n",
    "    \"dataset\": DataSet,\n",
    "    \"language\": \"en\",\n",
    "}\n",
    "\n",
    "trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdc628-c010-463f-90a0-af62e76dd5b5",
   "metadata": {},
   "source": [
    "### Apply captum_IG to test examples\n",
    "\n",
    "This produces the visualized annotation of all sentences (inline in the notebook) and several files in `OutDir`:\n",
    "  * `viz.html` a file with all of these annotations\n",
    "  * `stats.csv`a file with summary statistics for each test example, and\n",
    "  * `tokfreq.csv` a file with the distribution of tokens' attribution scores across all test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266fe780-70fe-4bd2-8e99-ff9fd8c9adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "captumIG(model,tokenizer,eval_TensorDict,egTarget,OutDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783d1d0-dd7e-4140-88b8-201d82c9625e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4law2",
   "language": "python",
   "name": "ai4law2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
